# Model settings
model_name: "stable-video-diffusion"
num_inference_steps: 25

# Optimization settings
optimizer: "adamw"
lr: 3.0e-6
beta1: 0.9
beta2: 0.95
batch_size: 1
accum_grad_steps: 25
gradient_clip: 1.0 #0.0 for no clipping, if positive, it is the max norm
gradient_clip_algorithm: "norm"

# Training strategy
precision: "bf16" #['32', 'bf16']
max_epochs: 100
num_timesteps_to_load_train: 2

# Paths
save_dir: "checkpoints"
train_video_path_json: "/gpfs-flash/junlab/yexi24-postdoc/soc-fine-tuning-sd/configs/refl_videos_pt.json"
val_video_path_json: "/gpfs-flash/junlab/yexi24-postdoc/soc-fine-tuning-sd/configs/refl_videos_val_pt.json"
target_vid_size: [24, 320, 576]
num_workers: 8
vid_data_type: "pt"

# Scheduler settings
scheduler: "linear_warmup"
warmup_steps: 20

# Model-specific settings
# beta_start: 0.002
# beta_end: 0.009
reward_multiplier: 100.0 #tradeoff between KL-div reg and the reward, larger values 
reward_func: "VideoAestheticsReward"
vae_decode_chunk_size: 8

reward_model_config:
  aesthetic_model_config:
    clip_model: 'ViT-B-32'

guidance_in_adjoint_computation: false
guidance_in_control_computation: false
prompt_dropout: 0.2
quick_evaluation: true
per_sample_threshold_quantile: 0.9 #-1 #negative value to disable
learn_offset: false

# Lora config
lora_adapter_name: "motion_adapter"
only_lora_on_temporal_module: true
lora_precision: "bfloat16" # options: ["fp32", "bfloat16", "fp16"]
lora_config:
  r: 4
  lora_alpha: 4
  init_lora_weights: "gaussian"
  target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
resume_from_checkpoint: null

# Gradient smoothing settings
smooth_gradients: false
smooth_samples: 4
smooth_noise_std: 0.02
smooth_clipping_quantile: 0.85

# Misc
use_tf32: true
seed: 0
wandb_project: "AM-SVD-Aesthetics-buffer"
checkpoint_every_n_epochs: 1
val_check_interval: 1.0
check_val_every_n_epoch: 10
verbose: false

# Buffer
use_buffer: true
buffer_device: cpu
buffer_size: 4
passes_per_buffer: 10  # Number of training passes through each buffer before refreshing