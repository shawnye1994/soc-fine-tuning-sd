# Model settings
model_name: "stable-diffusion-v1-5"
num_inference_steps: 50

# Optimization settings
optimizer: "adamw"
lr: 1.0e-5
beta1: 0.9
beta2: 0.95
batch_size: 5
accum_grad_steps: 25
gradient_clip: 0.0 #0.0 for no clipping, if positive, it is the max norm
gradient_clip_algorithm: "norm"

# Training strategy
eta: 1.0
precision: "32"
max_epochs: 10
num_timesteps_to_load: 50
num_timesteps_to_load_train: 4

# Paths
save_dir: "checkpoints"
training_prompt_path: "prompt_files/refl_data.json"
validation_prompt_path: "prompt_files/benchmark_ir.json"

# Scheduler settings
scheduler: "linear_warmup"
warmup_steps: 20

# Model-specific settings
beta_start: 0.002
beta_end: 0.009
reward_multiplier: 100.0
reward_func: "ImageReward"
guidance_scale: 7.5
guidance_in_adjoint_computation: false
guidance_in_control_computation: false
prompt_dropout: 0.2
quick_evaluation: true
per_sample_threshold_quantile: 0.9 #-1 #negative value to disable
learn_offset: false

# Gradient smoothing settings
smooth_gradients: false
smooth_samples: 20
smooth_noise_std: 0.02
smooth_clipping_quantile: 0.85

# Misc
use_tf32: true
seed: 0
wandb_project: "AM-SD15-prompt-993"
checkpoint_every_n_epochs: 1
val_check_interval: 0.1
resume_from_checkpoint: null
verbose: false

# Buffer
use_buffer: false
buffer_device: cpu
buffer_size: 100
passes_per_buffer: 10  # Number of training passes through each buffer before refreshing